---
layout: post
title:  "Exploratory Data Analysis of Board Documents at The Green Climate Fund"
date:   2017-4-26 17:55:01 +0900
category: r
tags: [r]
comments: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning=FALSE)
options(width = 100, dplyr.width = 100)
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(tidyr)
library(dplyr)
library(tidytext)
library(purrr)
library(stringr)
library(ggplot2)
library(scales)
library(gridExtra)
library(devtools)
library(gcfboardr)
library(ggraph)
library(igraph)
data("gcfboard_docs")
gcfboard_docs <- select(gcfboard_docs, c(3, 2, 1))
```

# Introducing gcfboardr 

In a previous post, I introduced `gcfboardr` a package I'd written which holds Green Climate Fund (GCF) board documents, ready for for text analysis.

By looking at the most important GCF documents we might be able to get a sense of priorities at the Fund, including changes over time.

I recommend using this data with the `tidytext` R package. Combining several smaller functions from `tidytext` and `dplyr` allows you to produce powerful transformations of text data. This is exactly what I'm going to do below.

### How to install

To install the gcfboardr package you'll need to have the devtools package installed, and then install gcfboardr from github using the following code:

```{r direction, eval=FALSE, echo=TRUE}
# Install gcfboardr from github. Note: this step can take 1-2 minutes.
library(devtools)
install_github("ljodea/gcfboardr") 

# Load the library and the data
library(gcfboardr)
data("gcfboard_docs")
```

## What's in the gcfboardr data set?

Let's load up a few libraries and take a glimpse at the data:

```{r sneakpeek, echo=TRUE}
library(dplyr)
library(ggplot2)
library(tidytext)

glimpse(gcfboard_docs)
```

We have almost 500,000 observations of three variables, in which every observation is a line from an original document. How many documents are there per meeting?

```{r docspm, echo=FALSE}
# How many docs per meeting?
gcfboard_docs %>%
  group_by(meeting) %>%
  summarise(docs = n_distinct(title)) %>% 
  ggplot(aes(meeting, docs)) +
  geom_bar(stat = "identity") +
  labs(title = "Documents produced per board meeting")

```

We can see that B.08 and B.11 were particularly prolific, and the early meetings produced far less text than the later ones. 
B.08 was the first board meeting of 2014, the year the Fund started operations, and it produce more than double the documents from the previous meeting! 

### Are board meetings reasonable groupings of text suitable for analysis?

When we're first looking at this data set, we might want to know whether our data is normal for a large corpus, and Zipf's Law is one yardstick we might use. Zipf's Law states that given a corpus of natural language text, the frequency of any word is inversely proportional to its rank in the frequency table. This is a power law, and it implies that the most common word appears roughly twice as often in a corpus than the 2nd most common word, and three times as often as the 3rd most common word, and so on. 

If this relationship holds for our data, we can proceed. If it doesn't we might have some problematic documents (this was the case for B.03, which has some badly formatted text). Let's check the data, grouping by board meeting. 


```{r zipf}
# Zipf's law ------
library(tidytext)
library(stringr)

bm_words <- gcfboard_docs %>%
  mutate(text = str_replace_all(text, "[[:digit:]]", "")) %>% 
  unnest_tokens(word, text) %>%
  count(meeting, word, sort = TRUE) %>%
  ungroup()

total_words <- bm_words %>%
  group_by(meeting) %>%
  summarize(total = sum(n))

bm_words <- left_join(bm_words, total_words) # this is purely to calculate Zipf's law

freq_by_rank <- bm_words %>%
  group_by(meeting) %>%
  mutate(rank = row_number(),
         `term frequency` = n/total)
freq_by_rank %>%
  ggplot(aes(rank, `term frequency`, color = meeting)) +
  geom_line(size = 1.2, alpha = 0.8) +
  geom_abline(intercept = -0.62, slope = -1.1, color = "gray50", linetype = 2) +
  scale_x_log10() +
  scale_y_log10() # B.04 probably contains misspelt or mis-read words

```

It looks like each group of board meeting documents in our data set obeys Zipf's law! This means that, without reading any documents, groups of documents belonging to each board meeting conform to out expectations about what a corpus should look like. There are no significant distortions in the text and we can move on to more interesting analysis.


## Single Word Analysis

So far we've been looking at lines, documents and meetings, but what we'd really like to look at is the words themselves. So let's unnest the words from their lines and remove some "stop words" -- words such as "and" "the" or "a" using an `anti_join` from the dplyr package.

```{r munging, echo=TRUE, message=FALSE}
# Load a table of 1,149 common stop words
data(stop_words)

# Filter out numbers and empty lines, then unnest tokens and anti_join a table of stop words
gcf_tidy <- gcfboard_docs %>%
  mutate(text = str_replace_all(text, "[[:digit:]]", "")) %>% 
  filter(text != "") %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words)
glimpse(gcf_tidy)

```

Even after we removed a list of 1,149 common stop words, the new tidy dataframe contains over 2.6 million words! Now let's look at the counts of remaining words, sorted by the number of times they appear in the text. 

```{r common words}
# Which words are most common?
gcf_tidy %>%
  count(word, sort = TRUE) %>%
  filter(n > 13600) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = n)) +
  geom_col(show.legend = FALSE) +
  xlab(NULL) +
  scale_fill_gradient() +
  coord_flip() +
  labs(title = "Top words by count in GCF Board Meeting Documents")
```

It looks like some words appear very often, in particular words associated with the name of the fund, although the word "green" is relatively less used than are the words "climate" and "fund".

### How many times does the phrase "Green Climate Fund" appear?

We might want to disambiguate uses of the word "climate" between uses which add important context to our analysis, and uses which just repeat the name of the fund.

To solve this problem, we're going to to see how many times the words "Green Climate Fund" appear in sequence:

```{r gcftrigram}
# How many times does the trigram "Green Climate Fund" appear?
gcf_name_trigram <- gcfboard_docs %>%
  mutate(text = str_replace_all(text, "[[:digit:]]", "")) %>% 
  filter(text != "") %>% 
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(word1 == "green",
         word2 == "climate",
         word3 == "fund") %>%
  count(word1, word2, word3, sort = TRUE)

gcf_name_trigram
```

Now we can see that 6,310 uses of each word "green", "climate" and "fund" are repetitions of the fund name. So how many uses are there in other contexts?

```{r gcfnamecount}
gcf_name_disambiguation <- gcf_tidy %>%
  count(word, sort = TRUE) %>%
  filter(word == "green" | word == "climate" | word == "fund") %>%
  transmute(word,
         `basic count` = n,
         `name context` = 6310,
         `other contexts` = `basic count` - `name context`)
gcf_name_disambiguation
```

We can see that "green" occurs rarely in contexts other than the name context. The word "green" is almost never used unless the name of the fund is repeated. 

Now we can find out the relative importance of the word "climate", disambiguated from usage which is just a repetition of the fund's name. 


```{r tidycount2, echo=FALSE, message=FALSE}
# Add custom stop words
gcf_stop_words <- bind_rows(stop_words, 
                           data_frame(word = c("green", "climate", "fund", "board", "gcf", "gcfb", "page"), 
                                      lexicon = rep("custom", 7)))

gcf_name_disambiguation <- gcf_name_disambiguation %>% 
  rename(n = `other contexts`) %>% 
  select(word, n)

# Filter out numbers, any empty lines, unnest tokens and anti_join custom stop words
gcf_tidy2 <- gcfboard_docs %>%
  mutate(text = str_replace_all(text, "[[:digit:]]", "")) %>% 
  filter(text != "") %>% 
  unnest_tokens(word, text) %>% 
  anti_join(gcf_stop_words) %>%
  count(word, sort = TRUE) %>%
  full_join(gcf_name_disambiguation)
  

# Which are the most common words, now that we've removed some gcf-specific words?
gcf_tidy2 %>%
  filter(n > 12000) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = n)) +
  geom_col(show.legend = FALSE) +
  xlab(NULL) +
  scale_fill_gradient() +
  coord_flip() +
  labs(title = "Top words by count in GCF Board Meeting Documents, Filtered")

```

Wow! It turns out that the word "climate", independent of usage as part of the name of the Fund, is the third most common word. It's a good job we didn't remove it out of hand. 

You might wonder why we didn't remove the word "project" since it is so common. This is because frequency of usage has changed a lot over time, unlike the other words we removed.

### How has word usage changed over time?

```{r wbm}
# Comparing word usage by meeting
words_by_meeting <- gcf_tidy %>%
  count(meeting, word) %>%
  ungroup() %>%
  group_by(meeting) %>%
  mutate(meeting_total = sum(n)) %>%
  group_by(word) %>%
  mutate(word_total = sum(n)) %>%
  ungroup() %>%
  rename(count = n) %>%
  filter(word_total > 500 | count > 20)

# Remove effect of meeting number on word usage
wbmlog <- words_by_meeting %>% 
  mutate(lwords = log2(count),
         meeting_num = unlist(str_extract_all(meeting, "\\d+"))) %>% 
  mutate(lmeeting = log10(as.numeric(meeting_num)))

mod <- lm(lwords ~ lmeeting, data = wbmlog)
```


Let's look at changes in usage for a few words which might be interesting to us: "secretariat", "project" and "risk".   

Plotting statistical transformations of word frequency can help us see these changes over time. Below you can see the bare frequenciess on top, followed by a log10 scale in the middle, and finally a squre root coordinate transform beneath.


```{r changes, fig.width=10, fig.height=15}
# Usage over time for selected words
changes <- filter(words_by_meeting, word %in% c("secretariat", "project", "risk")) %>%
  filter(meeting != "B.04") %>% 
  ggplot(aes(meeting, count/meeting_total, color = word)) +
  geom_point(size = 1.3) +
  geom_line(aes(group = word)) +
  labs(x = NULL, y = "Word frequency") 

c2 <- changes + scale_y_log10() + labs(x = NULL, y = "Log 10 of Word frequency") 
c3 <- changes + coord_trans(y = "sqrt") + labs(x = NULL, y = "Square Root of Word frequency") 

grid.arrange(changes, c2, c3, ncol = 1)
```

All plots show that since the Fund became operational in 2014, use of the word "project" has surged. 

At earlier meetings, establishing a secretariat was priority number one, and we can see usage declining over time. "Risk" begins to become much more important at B.07, which is when the Fund adopted a risk management framework.    

You'll notice that the bare frequency plot emphasises the high-frequency changes to the words "secretariat" and "project", whereas the log10 plot of word frequency really helps us see what's happening at the low-end of the frequency range. The square-root coordinate transform preserves some of the perspective of both the other plots, and might be the most useful plot of the three.

### What does single word analysis say about different board meetings?

We might want to look at board meetings as a facet of our single word analysis. Which words are particularly associated with specific meeetings? To find this out we can use common rules-of-thumb: term frequency and it's cousin, inverse-document frequency.

Computing this with the `bind_tf_idf` function from the tidytext package, we get a data table which shows the words which are associatd in particular with one meeting. 

```{r plotwbm, echo=FALSE}
bm_words <- bm_words %>%
  bind_tf_idf(word, meeting, n)

bm_words %>%
  select(-total) %>%
  arrange(desc(tf_idf)) # Interesting!

plot_bm <- bm_words %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word))))
```

We can then plot this data, faceting by meeting and picking the top 5 terms associated with a particular meeting:

```{r plotwbm2, echo=FALSE, fig.height=12, fig.width=10}
plot_bm %>%
  group_by(meeting) %>%
  filter(word != "page" & word != "anxjointmgten.doc" & word != "ÿ" & meeting != "B.04") %>% 
  top_n(5) %>%
  ungroup %>%
  ggplot(aes(word, tf_idf, fill = factor(meeting))) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "Term Frequency - Inverse Document Frequency") +
  theme(axis.text.x = element_blank()) +
  theme(axis.ticks.x = element_blank()) +
  coord_flip() +
  facet_wrap(~meeting, ncol = 4, scales = "free")
```

That's a lot of acronyms! If we look into the board documents themselves, we can see that these acronyms tend to denote real organizations or operational units at the fund. For example:

* at B.16, the top term is "geeref", which stands for "Global Energy Efficiency and Renewable Energy Fund"
* and at B.06, where the top term is "iiu", this refers to the "Independent Integrity Unit" (IIU), which is one of three accountability units at the fund. 

This means that relative to other board meetings, the IIU and GEEREF were a particularly important topics at the sixth and sixteenth meetings of the board respectively.

### How similar are individual board meetings?

We can also look at frequencies of words between meeting pairs. Words that are common to one meeting are common to another, and cancel each other out, leaving interesting words at the margins.

Let's use a 45 degree dashed line to denote equal frequency in word usage. This means that:

* words above the line appear more commonly in B.16 board documents than in the comparison group, while 
* words below the line appear more frequently in the comparison group than at B.16 (check the label above the plot to find which board meeting is used as the comparison). 

For example, in the first plot below we see that "co-chairs" was a big issue at B.01 (below the line), while it wasn't at B.16. In contrast, B.16 (above the line) had a greater focus on "women", "water" and "projects" than did B.01.

```{r frequencies, fig.height=15, fig.width=9}
frequency <- gcf_tidy %>% 
  mutate(word = str_extract(word, "[a-z']+")) %>%
  count(meeting, word) %>%
  filter(word != "board" & word != "gcf" & word != "green" & word != "fund" & word != "board") %>% 
  group_by(meeting) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  spread(meeting, proportion) %>% 
  gather(meeting, proportion, `B.15`:`B.01`)

frequency %>% 
  filter(meeting == "B.01" | meeting == "B.10" | meeting == "B.15" | meeting == "B.16") %>% 
  ggplot(aes(x = proportion, y = `B.16`, color = abs(`B.16` - proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  facet_wrap(~meeting, ncol = 1) +
  theme(legend.position="none") +
  labs(title = "Word Correlations: B.16 (y) and Other Meetings (x)", y = "B.16", x = NULL)
```

We can see that:

* B.01 was focused on finding a "host" country and establishing a "secretariat",
* B.10 was relatively more concerned with "accountability" and "accreditation" than was B.16,
* B.15 was relatively more concerned with "applicants" to the fund and contains many references to "samoa", where the meeting was held.

By looking at the spread of the data, we can also see that correlation between word frequencies at B.16 and at earlier meetings converges over time. Between B.16 and B.01 the word frequencies are dispered, while the B.16-B.10 pair is a little less dispersed, and the frequencies for B.15 appear to converge towards those for B.16.

We can confirm this intuition by looking at Pearson's product-moment correlations between B.16 and previous meetings:

* First between **B.01 and B.16**: correlation is about 32%
```{r}
cor.test(data = frequency[frequency$meeting == "B.01",],
         ~ proportion + `B.16`)
```

* Next between **B.10 and B.16**: correlation is about 62%
```{r}
cor.test(data = frequency[frequency$meeting == "B.10",],
         ~ proportion + `B.16`) 
```

* Last, between **B.15 and B.16**: correlation is about 92%
```{r}
cor.test(data = frequency[frequency$meeting == "B.15",],
         ~ proportion + `B.16`) 
```

Great! This confirms our intuition that B.16 is more similar to the meetings which immediately preceded it than to other earlier meetings of the board.

### Wrap Up

That's enough for one post! For more ideas about how to analyze this data set beyond single-word analysis, and for a lot more on correlation, standy by for my next post on ngrams. I will post the link here as soon as it's available.

***

### Notes

* Zipf's Law is a simple power law:

$$f(k;s,N)=\frac{1/k^s}{\sum_{n=1}^N (1/n^s)}$$

* **Term frequency (tf)**, measures how frequently a word occurs in a document. However, even after we remove words such as “and”, “the”, "a”, et cetera, there are some words which will occur much more often than others, so it's not much use on its own.

$$tf(\text{word}) = (\frac{n_{\text{word}}}{n_{\text{total words in document}}})$$  

* **Inverse document frequency (idf)** weights the frequency of a word by how rarely it occurs in a set of documents, and increases the weight for words that are seldom seen in the set. You'll notice that since this is a logarithm, an extremely common word would get a rating of zero.  

$$idf(\text{word}) = {\ln{\left(\frac{n_{\text{documents}}}{n_{\text{documents containing term}}}\right)}}$$  

* **Term frequency inverse document frequency (tf-idf)** combines the above two concepts. You can think about this as the frequency of a term in a document weighted by how rarely it is used among a group of documents, with higher weights going to rarely used words, and lower weights to commonly-used words.   
  
$$tfidf(\text{word}) = tf(\text{word}) * idf(\text{word})$$  




