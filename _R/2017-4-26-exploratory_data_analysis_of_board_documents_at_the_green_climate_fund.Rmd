---
layout: post
title:  "Exploratory Data Analysis: The Green Climate Fund"
date:   2017-4-26 17:55:01 +0900
category: r
tags: [r]
comments: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning=FALSE)
options(width = 100, dplyr.width = 100)
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(tidyr)
library(dplyr)
library(tidytext)
library(purrr)
library(stringr)
library(ggplot2)
library(scales)
library(gridExtra)
library(devtools)
library(gcfboardr)
library(ggraph)
library(igraph)
data("gcfboard_docs")
gcfboard_docs <- select(gcfboard_docs, c(3, 2, 1))
gcfboard_docs <- gcfboard_docs %>%
  mutate(text = str_replace_all(text, "[[:digit:]]", "")) %>% 
  filter(text != "")
```

Recently, I harvested over 500,000 lines of text from 500+ Green Climate Fund board documents. Because of this, I've  built a data-only R package called `gcfboardr`, so that anyone can make use of the corpus I've created.

To build the data, I used documents produced for board meetings, available [here on the GCF website](http://http://www.greenclimate.fund/boardroom/board-meetings/documents). I've read some of these documents before, and it occured me that the Fund will produce more text than anyone can read in a lifetime. So I've used my natural curiosity about the Fund as a motivating project with which to practice the tidytext approach to text analysis in R and gain deeper insight into the Fund.

In this post, I'm going to show what can be done with gcfboardr.

***

### Installation

To install the gcfboardr package you'll need to have the devtools package installed, and then install gcfboardr from github using the following code:

```{r direction, eval=FALSE, echo=TRUE}
# Install gcfboardr from github. Note: this step can take 1-2 minutes.
library(devtools)
install_github("ljodea/gcfboardr") 

# Load the library and the data
library(gcfboardr)
data("gcfboard_docs")
```

***

### Preview

Let's load up a few libraries and take a glimpse at the data:

```{r sneakpeek, echo=TRUE}
library(dplyr)
library(ggplot2)
library(tidytext)

glimpse(gcfboard_docs)
```

We have almost 500,000 observations of three variables, in which every observation is a line from an original document. How many documents are there per meeting?

```{r docspm, echo=FALSE}
# How many docs per meeting?
gcfboard_docs %>%
  group_by(meeting) %>%
  summarise(docs = n_distinct(title)) %>% 
  ggplot(aes(meeting, docs)) +
  geom_bar(stat = "identity") +
  labs(title = "Documents produced per board meeting")
```

We can see that B.08 and B.11 were particularly prolific, and the early meetings produced fewer docs than the later ones. B.08 was the third board meeting of 2014, the year the Fund started operations, and it produced more than double the documents from the previous meeting!

***

## EDA of One Variable: Single Words

So far we've been looking at lines, documents and meetings, but what we'd really like to look at is the words themselves. So let's unnest the words from their lines and remove some "stop words" -- words such as "and" "the" or "a" -- using an `anti_join` from the dplyr package.

```{r munging, echo=TRUE, message=FALSE}
# Load a table of 1,149 common stop words
data(stop_words)

# Unnest word tokens and anti_join a table of stop words
gcf_tidy <- gcfboard_docs %>%
  unnest_tokens(word, text) %>% 
  anti_join(stop_words)
glimpse(gcf_tidy)
```

Even after we removed every instance of common stop words, the new tidy dataframe contains over 2.6 million words! Now let's look at the counts of remaining words, sorted by the number of times they appear in the text. 

```{r common words}
# Which words are most common?
gcf_tidy %>%
  count(word, sort = TRUE) %>%
  filter(n > 13600) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = n)) +
  geom_col(show.legend = FALSE) +
  xlab(NULL) +
  scale_fill_gradient() +
  coord_flip() +
  labs(title = "Top words by count in GCF Board Meeting Documents") +
  theme(axis.text=element_text(size=12),
        axis.title=element_text(size=14,face="bold")) 
```

It looks like some words appear very often, in particular words associated with the name of the fund, although the word "green" is relatively less used than are the words "climate" and "fund".

We might want to disambiguate uses of the word "climate" between uses which add important context to our analysis, and uses which just repeat the name of the fund.

To solve this problem, we're going to to see how many times the words "Green Climate Fund" appear in sequence:

```{r gcftrigram, echo=TRUE}
# How many times does the trigram "Green Climate Fund" appear?
gcf_name_trigram <- gcfboard_docs %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(word1 == "green",
         word2 == "climate",
         word3 == "fund") %>%
  count(word1, word2, word3, sort = TRUE)
gcf_name_trigram
```

Now we can see that 6,310 uses of each word "green", "climate" and "fund" are repetitions of the fund name. So how many uses are there in other contexts?

```{r gcfnamecount, echo=TRUE}
gcf_name_disambiguation <- gcf_tidy %>%
  count(word, sort = TRUE) %>%
  filter(word == "green" | word == "climate" | word == "fund") %>%
  transmute(word,
         `basic count` = n,
         `name context` = 6310,
         `other contexts` = `basic count` - `name context`)
gcf_name_disambiguation
```

We can see that "green" occurs rarely in contexts other than the name context. The word "green" is almost never used unless the name of the fund is repeated. 

Now we can find out the relative importance of the word "climate", disambiguated from usage which is just a repetition of the fund's name. 

```{r tidycount2, echo=FALSE, message=FALSE}
# Add custom stop words
gcf_stop_words <- bind_rows(stop_words, 
                           data_frame(word = c("green", "climate", "fund", "board", "gcf", "gcfb", "page"), 
                                      lexicon = rep("custom", 7)))

gcf_name_disambiguation <- gcf_name_disambiguation %>% 
  rename(n = `other contexts`) %>% 
  select(word, n)

# Unnest tokens and full-join the disambiguation table
gcf_tidy2 <- gcfboard_docs %>%
  unnest_tokens(word, text) %>% 
  anti_join(gcf_stop_words) %>%
  count(word, sort = TRUE) %>%
  full_join(gcf_name_disambiguation)
  
# Which are the most common words, now that we've removed some gcf-specific words?
gcf_tidy2 %>%
  filter(n > 12000) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = n)) +
  geom_col(show.legend = FALSE) +
  xlab(NULL) +
  scale_fill_gradient() +
  coord_flip() +
  labs(title = "Top words by count in GCF Board Meeting Documents, Filtered") +
  theme(axis.text=element_text(size=12),
        axis.title=element_text(size=14,face="bold")) 
```

Wow! It turns out that the word "climate", independent of usage as part of the name of the Fund, is the third most common word. 

You might wonder why we didn't remove the word "project" since it is so common. This is because frequency of usage has changed a lot over time, unlike the other words we removed.

***

### How has word usage changed over time?

```{r wbm}
# Comparing word usage by meeting
words_by_meeting <- gcf_tidy %>%
  count(meeting, word) %>%
  ungroup() %>%
  group_by(meeting) %>%
  mutate(meeting_total = sum(n)) %>%
  group_by(word) %>%
  mutate(word_total = sum(n)) %>%
  ungroup() %>%
  rename(count = n) %>%
  filter(word_total > 500 | count > 20)

# Remove effect of meeting number on word usage
wbmlog <- words_by_meeting %>% 
  mutate(lwords = log2(count),
         meeting_num = unlist(str_extract_all(meeting, "\\d+"))) %>% 
  mutate(lmeeting = log10(as.numeric(meeting_num)))

mod <- lm(lwords ~ lmeeting, data = wbmlog)
```


Let's look at changes in usage for a few words which might be interesting to us: "secretariat", "project" and "risk".

Plotting statistical transformations of word frequency can help us see these changes over time. Below you can see the bare frequenciess on top, followed by a log10 scale in the middle, and finally a squre root coordinate transform beneath.


```{r changes, fig.width=10, fig.height=15}
# Usage over time for selected words
changes <- filter(words_by_meeting, word %in% c("secretariat", "project", "risk")) %>%
  filter(meeting != "B.04") %>% 
  ggplot(aes(meeting, count/meeting_total, color = word)) +
  geom_point(size = 1.3) +
  geom_line(aes(group = word)) +
  labs(x = NULL, y = "Word frequency") +
  theme(axis.text=element_text(size=12),
        axis.title=element_text(size=14,face="bold"),
        legend.text =element_text(size=12))

c2 <- changes + scale_y_log10() + labs(x = NULL, y = "Log 10 of Word frequency") 
c3 <- changes + coord_trans(y = "sqrt") + labs(x = NULL, y = "Square Root of Word frequency") 

grid.arrange(changes, c2, c3, ncol = 1)
```

All plots show that since the Fund became operational in 2014, use of the word "project" has surged. 

At earlier meetings, establishing a secretariat was priority number one, and we can see usage declining over time. "Risk" begins to become much more important at B.07, which is when the Fund adopted an investment framework, a financial risk management framework, and a results management framework.

You'll notice that the bare frequency plot emphasises the high-frequency changes to the words "secretariat" and "project", whereas the log10 plot of word frequency really helps us see what's happening at the low-end of the frequency range. The square-root coordinate transform preserves some of the perspective of both the other plots, and might be the most useful plot of the three.

***

### Word Frequency

We might want to look at board meetings as a facet of our single word analysis. Which words are particularly associated with specific meeetings? To find this out we can use common rules-of-thumb: term frequency and it's cousin, inverse-document frequency.

Computing these with the `bind_tf_idf` function from the tidytext package, we get a data table which shows the words which are associatd in particular with one meeting. 

```{r plotwbm, echo=FALSE}
bm_words <- gcfboard_docs %>%
  mutate(text = str_replace_all(text, "[[:digit:]]", "")) %>% 
  unnest_tokens(word, text) %>%
  count(meeting, word, sort = TRUE) %>%
  ungroup()

total_words <- bm_words %>%
  group_by(meeting) %>%
  summarize(total = sum(n))

bm_words <- left_join(bm_words, total_words) # this is purely to calculate Zipf's law

bm_words <- bm_words %>%
  bind_tf_idf(word, meeting, n)

bm_words %>%
  select(-total) %>%
  arrange(desc(tf_idf)) # Interesting!

plot_bm <- bm_words %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word))))
```

We can then plot this data, faceting by meeting and picking the top 5 terms associated with a particular meeting:

```{r plotwbm2, echo=FALSE, fig.height=10, fig.width=10}
plot_bm %>%
  group_by(meeting) %>%
  filter(word != "page" & word != "anxjointmgten.doc" & word != "ÿ" & meeting != "B.04") %>% 
  top_n(5) %>%
  ungroup %>%
  ggplot(aes(word, tf_idf, fill = factor(meeting))) +
  geom_col(show.legend = FALSE) +
  labs(title = "Top 5 Words By Term Frequency - Inverse Document Frequency, Faceted by Meeting", x = NULL, y = NULL) +
  theme(axis.text.x = element_blank()) +
  theme(axis.ticks.x = element_blank()) +
  theme(axis.text=element_text(size=12),
        axis.title=element_text(size=14,face="bold"),
        legend.text =element_text(size=12),
        strip.text.x = element_text(size = 12, face="bold")) + 
  coord_flip() +
  facet_wrap(~meeting, ncol = 4, scales = "free")
```

That's a lot of acronyms! If we look into the board documents themselves, we can see that these acronyms tend to denote real organizations or operational units at the fund. For example:

* At B.16, the top term is "geeref", which is the ["Global Energy Efficiency and Renewable Energy Fund"](http://geeref.com/), a Public-Private Partnership which makes equity investments in clean energy in developing countries.
* At B.06, where the top term is "iiu", this refers to the ["Independent Integrity Unit" (IIU)](http://www.greenclimate.fund/independent-integrity-unit), which is one of three accountability units at the fund. 

This means that relative to other board meetings, the IIU and GEEREF were a particularly important topics at the sixth and sixteenth meetings of the board respectively.

***

### Word Correlation

We can also look at correlation in word frequency between pairs of meetings. Words that are common to one meeting are common to another, and cancel each other out, leaving interesting words at the margins.

Let's use a 45 degree dashed line to denote equal frequency in word usage. This means that:

* words above the line appear more commonly in B.16 board documents than in the comparison group, while 
* words below the line appear more frequently in the comparison group than at B.16 (check the label above the plot to find which board meeting is used as the comparison). 

For example, in the first plot below we see (below the line) that "co-chairs" was a big issue at B.01, while it wasn't at B.16. In contrast, B.16 (above the line) had a greater focus on "women", "water" and "projects" than did B.01.

```{r frequencies, fig.height=15, fig.width=9}
frequency <- gcf_tidy %>% 
  mutate(word = str_extract(word, "[a-z']+")) %>%
  count(meeting, word) %>%
  filter(word != "board" & word != "gcf" & word != "green" & word != "fund" & word != "board") %>% 
  group_by(meeting) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  spread(meeting, proportion) %>% 
  gather(meeting, proportion, `B.15`:`B.01`)

frequency %>% 
  filter(meeting == "B.01" | meeting == "B.10" | meeting == "B.15" | meeting == "B.16") %>% 
  ggplot(aes(x = proportion, y = `B.16`, color = abs(`B.16` - proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  facet_wrap(~meeting, ncol = 1) +
  theme(legend.position="none") +
  labs(title = "Word Correlations: B.16 (y) ~ Other Meetings (x)", y = "B.16", x = NULL) +
  theme(axis.text=element_text(size=12),
        axis.title=element_text(size=14,face="bold"),
        legend.text =element_text(size=12),
        strip.text = element_text(size=14,face="bold"))
```

We can also see that:

* B.01 was focused on finding a "host" country and establishing a "secretariat",
* B.10 was relatively more concerned with "accountability" and "accreditation" than was B.16,
* B.15 was relatively more concerned with "applicants" to the fund and contains many references to "samoa", where the meeting was held.

By looking at the spread of the data, we can also see that correlation between word frequencies at B.16 and at earlier meetings converges over time. Between B.16 and B.01 the word frequencies are dispered, while the B.16-B.10 pair is a little less dispersed, and the frequencies for B.15 appear to converge towards those for B.16.

We can confirm this intuition by looking at Pearson's product-moment correlations between B.16 and previous meetings:

* between **B.01 and B.16**: correlation is about 32%
```{r}
cor.test(data = frequency[frequency$meeting == "B.01",],
         ~ proportion + `B.16`)
```

* between **B.10 and B.16**: correlation is about 62%
```{r}
cor.test(data = frequency[frequency$meeting == "B.10",],
         ~ proportion + `B.16`) 
```

* between **B.15 and B.16**: correlation is about 92%
```{r}
cor.test(data = frequency[frequency$meeting == "B.15",],
         ~ proportion + `B.16`) 
```

Great! This confirms our intuition that B.16 is more similar to the meetings which immediately preceded it than to other earlier meetings of the board.

### Wrap Up

That's enough for one post! For more ideas about how to analyze this data set, and [for a lot more on correlation, see my post on ngrams](http://state.gy/r/ngrams_correlation_green_climate_fund/). 

***

### Notes

* **Term frequency (tf)**, measures how frequently a word occurs in a document. However, even after we remove stop words, there are always some words which will occur far more often than others, so it's not much use on its own.

$$tf(\text{word}) = (\frac{n_{\text{word}}}{n_{\text{total words in document}}})$$  

* **Inverse document frequency (idf)** weights the frequency of a word by how rarely it occurs in a set of documents, and increases the weight for words that are seldom seen in the set. You'll notice that since this is a logarithm, an extremely common word would get a rating of zero.  

$$idf(\text{word}) = {\ln{\left(\frac{n_{\text{documents}}}{n_{\text{documents containing term}}}\right)}}$$  

* **Term frequency inverse document frequency (tf-idf)** combines the above two concepts. You can think about this as the frequency of a term in a document weighted by how rarely it is used among a group of documents, with higher weights going to rarely used words, and lower weights to commonly-used words.   
  
$$tfidf(\text{word}) = tf(\text{word}) * idf(\text{word})$$  




